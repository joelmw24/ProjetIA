{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Préparation des Données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des données de 1 :  (7409, 20)\n",
      "Dimensions des données de 2 :  (7409, 7)\n",
      "Dimensions des données de 3 :  (192, 7)\n",
      "Dimensions des données de 4 :  (192, 7)\n",
      "    longitude   latitude  haut_tot  haut_tronc  tronc_diam  age_estim  \\\n",
      "0    3.290751  49.848312  1.147136    0.175604    3.420865   7.121579   \n",
      "1    3.314179  49.843834 -0.971321   -0.617867   -1.479382  -1.185090   \n",
      "2    3.280806  49.865129 -0.838918   -1.411339   -1.401601  -1.185090   \n",
      "3    3.286612  49.844846  0.087907    2.159284   -0.794903  -0.511576   \n",
      "4    3.297266  49.862782  0.749925   -0.221131    0.387378  -0.062567   \n",
      "5    3.297946  49.864024 -0.309303   -1.411339   -0.903798  -0.511576   \n",
      "6    3.294984  49.850041  1.014732    0.175604    1.320759   1.733470   \n",
      "7    3.292854  49.851386  1.544346    2.556020    2.409703   3.978515   \n",
      "8    3.293294  49.851225  2.073960    3.746228    1.242977   1.733470   \n",
      "9    3.294641  49.850508  1.014732    3.746228    0.931850   1.733470   \n",
      "10   3.310065  49.855100 -0.176900    0.175604   -0.234875  -0.062567   \n",
      "11   3.304378  49.862207  0.352714   -0.221131    0.387378  -0.062567   \n",
      "12   3.263364  49.836235  0.352714    0.572340   -0.079312  -0.062567   \n",
      "13   3.314832  49.839461 -0.971321    0.175604   -0.623784  -0.062567   \n",
      "14   3.308175  49.844482  0.087907   -0.617867   -0.157094   0.161938   \n",
      "15   3.281706  49.841278  1.411943   -0.221131    0.465160   0.386442   \n",
      "16   3.301660  49.864745  1.279539    0.572340    0.154033  -0.511576   \n",
      "17   3.263491  49.836570  0.882328    0.572340    0.309597  -0.062567   \n",
      "18   3.299257  49.863206  0.087907    0.969076    0.465160   0.161938   \n",
      "19   3.263416  49.836441  0.882328    0.572340    0.698505  -0.062567   \n",
      "20   3.284725  49.846579 -0.574110   -0.221131   -1.090474   0.386442   \n",
      "21   3.308703  49.843919  1.279539   -0.221131    0.931850   0.835451   \n",
      "22   3.295191  49.860386 -1.236128   -0.617867   -0.934911  -0.511576   \n",
      "23   3.300817  49.860933 -0.309303    0.572340   -0.468220  -0.511576   \n",
      "24   3.263330  49.836315  0.485118    0.572340   -0.079312  -0.062567   \n",
      "25   3.300026  49.861394 -0.309303    0.175604   -0.468220  -0.062567   \n",
      "26   3.321384  49.835840  1.279539   -0.221131    0.465160   0.386442   \n",
      "27   3.310778  49.845185  1.014732    0.572340    1.009632   0.386442   \n",
      "28   3.322955  49.837616 -1.368532   -0.221131   -1.246037  -0.511576   \n",
      "\n",
      "    fk_arb_etat  \n",
      "0             1  \n",
      "1             1  \n",
      "2             0  \n",
      "3             1  \n",
      "4             1  \n",
      "5             1  \n",
      "6             1  \n",
      "7             1  \n",
      "8             1  \n",
      "9             1  \n",
      "10            1  \n",
      "11            1  \n",
      "12            1  \n",
      "13            1  \n",
      "14            0  \n",
      "15            1  \n",
      "16            0  \n",
      "17            1  \n",
      "18            1  \n",
      "19            1  \n",
      "20            1  \n",
      "21            1  \n",
      "22            1  \n",
      "23            1  \n",
      "24            1  \n",
      "25            1  \n",
      "26            1  \n",
      "27            1  \n",
      "28            1  \n"
     ]
    }
   ],
   "source": [
    "# Extraction des données d’intérêt\n",
    "file_arbre = 'Data_Arbre.csv'\n",
    "arbre = pd.read_csv(file_arbre)\n",
    "print(\"Dimensions des données de 1 : \", arbre.shape)\n",
    "\n",
    "# Les colonnes pertinentes\n",
    "colonnes_pertinentes = [\"longitude\", \"latitude\", \"haut_tot\", \"haut_tronc\", \"tronc_diam\", \"fk_arb_etat\", \"age_estim\"]\n",
    "data_arbre = arbre[colonnes_pertinentes].copy()\n",
    "# Vérification des dimensions des données de test\n",
    "print(\"Dimensions des données de 2 : \", data_arbre.shape)\n",
    "\n",
    "# Filtrage pour inclure uniquement 'Essouché' et 'Non essouché'\n",
    "data_arbre = data_arbre[data_arbre['fk_arb_etat'].isin(['Essouché', 'Non essouché'])]\n",
    "\n",
    "# Vérification du nombre de chaque catégorie\n",
    "#print(\"Nombre de 'Essouché' : \", (data_arbre['fk_arb_etat'] == 'Essouché').sum())\n",
    "#print(\"Nombre de 'Non essouché' : \", (data_arbre['fk_arb_etat'] == 'Non essouché').sum())\n",
    "\n",
    "# Vérification des valeurs manquantes dans la colonne 'fk_arb_etat'\n",
    "#print(\"Nombre de valeurs manquantes dans 'fk_arb_etat' : \", data_arbre['fk_arb_etat'].isna().sum())\n",
    "\n",
    "# Transformation de 'fk_arb_etat' en variable binaire\n",
    "data_arbre['fk_arb_etat'] = (data_arbre['fk_arb_etat'] == 'Essouché').astype(int)\n",
    "\n",
    "# Sauvegarde des données filtrées\n",
    "with open('Encodage', 'wb') as f:\n",
    "    pickle.dump(data_arbre, f)\n",
    "\n",
    "print(\"Dimensions des données de 3 : \", data_arbre.shape)\n",
    "\n",
    "# Séparation des caractéristiques et de la cible\n",
    "x = data_arbre.drop(columns=['fk_arb_etat'])\n",
    "y = data_arbre['fk_arb_etat']\n",
    "\n",
    "print(\"Dimensions des données de 4 : \", data_arbre.shape)\n",
    "# Séparation en ensembles d'entraînement (60%), de validation (25%) et de test (15%)\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Colonnes des coordonnées\n",
    "coordonnes = ['longitude', 'latitude']\n",
    "\n",
    "# Séparer les colonnes des coordonnées pour ne pas les normaliser\n",
    "x_train_coords = x_train[coordonnes]\n",
    "x_val_coords = x_val[coordonnes]\n",
    "x_test_coords = x_test[coordonnes]\n",
    "x_train_features = x_train.drop(columns=coordonnes)\n",
    "x_val_features = x_val.drop(columns=coordonnes)\n",
    "x_test_features = x_test.drop(columns=coordonnes)\n",
    "\n",
    "# Mise à l'échelle des caractéristiques\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled_fit = scaler.fit_transform(x_train_features)\n",
    "x_val_scaled_fit = scaler.transform(x_val_features)\n",
    "x_test_scaled_fit = scaler.transform(x_test_features)\n",
    "\n",
    "# Sauvegarde du scaler pour la normalisation\n",
    "with open('Normalisation', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Convertir les résultats en DataFrame\n",
    "x_train_scaled_features = pd.DataFrame(x_train_scaled_fit, columns=x_train_features.columns, index=x_train.index)\n",
    "x_val_scaled_features = pd.DataFrame(x_val_scaled_fit, columns=x_val_features.columns, index=x_val.index)\n",
    "x_test_scaled_features = pd.DataFrame(x_test_scaled_fit, columns=x_test_features.columns, index=x_test.index)\n",
    "\n",
    "# Concaténer les données normalisées avec les coordonnées\n",
    "x_train_scaled = pd.concat([x_train_coords, x_train_scaled_features], axis=1)\n",
    "x_val_scaled = pd.concat([x_val_coords, x_val_scaled_features], axis=1)\n",
    "x_test_scaled = pd.concat([x_test_coords, x_test_scaled_features], axis=1)\n",
    "\n",
    "# Sauvegarde des données de test au format JSON\n",
    "test_data = pd.concat([x_test_scaled.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "test_data.to_json('test_data.json', orient='records')\n",
    "\n",
    "# Vérification des dimensions des données de test\n",
    "#print(\"Dimensions des données de test : \", test_data.shape)\n",
    "\n",
    "# Affichage des données de test et vérification des valeurs manquantes\n",
    "#print(\"Valeurs manquantes dans les données de test : \")\n",
    "#print(test_data.isna().sum())\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Apprentissage Supervisé pour la régression classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de précision pour chaque fold : [0.85365854 0.85365854 0.25      ]\n",
      "Précision moyenne : 0.6524390243902439\n",
      "Matrice de confusion :\n",
      " [[0. 1.]\n",
      " [0. 1.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.83      1.00      0.91        34\n",
      "\n",
      "    accuracy                           0.83        41\n",
      "   macro avg       0.41      0.50      0.45        41\n",
      "weighted avg       0.69      0.83      0.75        41\n",
      "\n",
      "Scores de précision pour chaque fold : [0.64285714 0.5        0.5       ]\n",
      "Précision moyenne : 0.5476190476190476\n",
      "Matrice de confusion :\n",
      " [[1.         0.        ]\n",
      " [0.73076923 0.26923077]]\n",
      "\n",
      "Rapport de classification :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      1.00      0.24         3\n",
      "           1       1.00      0.27      0.42        26\n",
      "\n",
      "    accuracy                           0.34        29\n",
      "   macro avg       0.57      0.63      0.33        29\n",
      "weighted avg       0.91      0.34      0.41        29\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred = sgd_clf.predict(x_val_scaled)\n",
    "\n",
    "# Calcul de la matrice de confusion sur l'ensemble de validation\n",
    "matrix = confusion_matrix(y_val, y_pred, normalize='true')\n",
    "\n",
    "# Validation croisée pour évaluer la performance du modèle\n",
    "scores = cross_val_score(sgd_clf, x_train_scaled, y_train, cv=3, scoring='accuracy')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Scores de précision pour chaque fold :\", scores)\n",
    "print(\"Précision moyenne :\", scores.mean())\n",
    "print(\"Matrice de confusion :\\n\", matrix)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "#  SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train_res, y_train_res = sm.fit_resample(x_train_scaled, y_train)\n",
    "\n",
    "# Entraînement du modèle sur les données résamplées\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(x_train_res, y_train_res)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = sgd_clf.predict(x_test_scaled)\n",
    "\n",
    "# Validation croisée pour évaluer la performance du modèle sur les données résamplées\n",
    "scores = cross_val_score(sgd_clf, x_train_res, y_train_res, cv=3, scoring='accuracy')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Scores de précision pour chaque fold :\", scores)\n",
    "print(\"Précision moyenne :\", scores.mean())\n",
    "print(\"Matrice de confusion :\\n\", confusion_matrix(y_test, y_pred,normalize='true'))\n",
    "print(\"\\nRapport de classification :\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de précision pour chaque fold (validation) : [0.85365854 0.73170732 0.875     ]\n",
      "Précision moyenne (validation) : 0.8201219512195123\n",
      "Matrice de confusion (validation) :\n",
      " [[ 1  6]\n",
      " [ 1 33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.14      0.22         7\n",
      "           1       0.85      0.97      0.90        34\n",
      "\n",
      "    accuracy                           0.83        41\n",
      "   macro avg       0.67      0.56      0.56        41\n",
      "weighted avg       0.79      0.83      0.79        41\n",
      "\n",
      "Scores de précision pour chaque fold : [0.88571429 0.85714286 0.88571429]\n",
      "Précision moyenne : 0.8761904761904762\n",
      "Matrice de confusion (test) :\n",
      " [[ 1  2]\n",
      " [ 2 24]]\n",
      "\n",
      "Rapport de classification (test) :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.33      0.33         3\n",
      "           1       0.92      0.92      0.92        26\n",
      "\n",
      "    accuracy                           0.86        29\n",
      "   macro avg       0.63      0.63      0.63        29\n",
      "weighted avg       0.86      0.86      0.86        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "gb_clf.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de validation\n",
    "y_pred_val = gb_clf.predict(x_val_scaled)\n",
    "\n",
    "# Calcul de la matrice de confusion sur l'ensemble de validation\n",
    "matrix_val = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "# Validation croisée pour évaluer la performance du modèle\n",
    "scores_val = cross_val_score(gb_clf, x_train_scaled, y_train, cv=3, scoring='accuracy')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Scores de précision pour chaque fold (validation) :\", scores_val)\n",
    "print(\"Précision moyenne (validation) :\", scores_val.mean())\n",
    "print(\"Matrice de confusion (validation) :\\n\", matrix_val)\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# Résampling avec SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train_res, y_train_res = sm.fit_resample(x_train_scaled, y_train)\n",
    "\n",
    "# Utilisation de GradientBoostingClassifier pour le modèle après résampling\n",
    "gb_res = GradientBoostingClassifier(random_state=42)\n",
    "gb_res.fit(x_train_res, y_train_res)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred_test = gb_res.predict(x_test_scaled)\n",
    "\n",
    "# Validation croisée pour évaluer la performance du modèle sur les données résamplées\n",
    "scores_res = cross_val_score(gb_res, x_train_res, y_train_res, cv=3, scoring='accuracy')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Scores de précision pour chaque fold :\", scores_res)\n",
    "print(\"Précision moyenne :\", scores_res.mean())\n",
    "print(\"Matrice de confusion (test) :\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"\\nRapport de classification (test) :\\n\", classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de précision pour chaque fold (SVC) : [0.85365854 0.85365854 0.875     ]\n",
      "Précision moyenne (SVC) : 0.8607723577235772\n",
      "Matrice de confusion (SVC) :\n",
      " [[ 0  3]\n",
      " [ 0 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.90      1.00      0.95        26\n",
      "\n",
      "    accuracy                           0.90        29\n",
      "   macro avg       0.45      0.50      0.47        29\n",
      "weighted avg       0.80      0.90      0.85        29\n",
      "\n",
      "Scores de précision pour chaque fold  : [0.6        0.58571429 0.57142857]\n",
      "Précision moyenne  : 0.5857142857142857\n",
      "Matrice de confusion (test) :\n",
      " [[1.         0.        ]\n",
      " [0.73076923 0.26923077]]\n",
      "\n",
      "Rapport de classification (test) :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      1.00      0.24         3\n",
      "           1       1.00      0.27      0.42        26\n",
      "\n",
      "    accuracy                           0.34        29\n",
      "   macro avg       0.57      0.63      0.33        29\n",
      "weighted avg       0.91      0.34      0.41        29\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modèle SVC\n",
    "svc_clf = SVC(random_state=42)\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement mises à l'échelle\n",
    "svc_clf.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred_svc = svc_clf.predict(x_test_scaled)\n",
    "\n",
    "# Calcul de la matrice de confusion sur l'ensemble de test\n",
    "matrix_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "\n",
    "# Validation croisée pour évaluer la performance du modèle\n",
    "scores_svc = cross_val_score(svc_clf, x_train_scaled, y_train, cv=3, scoring='accuracy')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Scores de précision pour chaque fold (SVC) :\", scores_svc)\n",
    "print(\"Précision moyenne (SVC) :\", scores_svc.mean())\n",
    "print(\"Matrice de confusion (SVC) :\\n\", matrix_svc)\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "# Résampling avec SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train_res, y_train_res = sm.fit_resample(x_train_scaled, y_train)\n",
    "\n",
    "sv_res = SVC(random_state=42)\n",
    "sv_res.fit(x_train_res, y_train_res)\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = sv_res.predict(x_test_scaled)\n",
    "\n",
    "# Validation croisée avec les données résamplées\n",
    "scores_res = cross_val_score(sv_res, x_train_res, y_train_res, cv=3, scoring='accuracy')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Scores de précision pour chaque fold  :\", scores_res)\n",
    "print(\"Précision moyenne  :\", scores_res.mean())\n",
    "print(\"Matrice de confusion (test) :\\n\", confusion_matrix(y_test, y_pred_test, normalize='true'))\n",
    "print(\"\\nRapport de classification (test) :\\n\", classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END .alpha=0.001, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END ..alpha=0.01, loss=hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=modified_huber, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, loss=squared_hinge, max_iter=3000, penalty=elasticnet; total time=   0.0s\n",
      "Meilleurs paramètres trouvés pour SGD :\n",
      "{'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1'}\n",
      "Meilleur score de validation croisée pour SGD :\n",
      "0.8853658536585366\n",
      "Rapport de classification sur l'ensemble de test pour SGD :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.90      1.00      0.95        26\n",
      "\n",
      "    accuracy                           0.90        29\n",
      "   macro avg       0.45      0.50      0.47        29\n",
      "weighted avg       0.80      0.90      0.85        29\n",
      "\n",
      "Enregistrement du modèle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "param_grid_sgd = {\n",
    "    'loss': ['hinge', 'modified_huber', 'squared_hinge'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "}\n",
    "\n",
    "grid_search_sgd = GridSearchCV(SGDClassifier(random_state=42), param_grid_sgd, cv=3, scoring='accuracy', verbose=2)\n",
    "grid_search_sgd.fit(x_train_scaled, y_train)  # entraînement \n",
    "\n",
    "print(\"Meilleurs paramètres trouvés pour SGD :\")\n",
    "print(grid_search_sgd.best_params_)\n",
    "print(\"Meilleur score de validation croisée pour SGD :\")\n",
    "print(grid_search_sgd.best_score_)\n",
    "\n",
    "best_model_sgd = grid_search_sgd.best_estimator_\n",
    "y_pred_sgd = best_model_sgd.predict(x_test_scaled)\n",
    "\n",
    "print(\"Rapport de classification sur l'ensemble de test pour SGD :\")\n",
    "print(classification_report(y_test, y_pred_sgd))\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "with open('best_sgd_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model_sgd, file)\n",
    "print(\"Enregistrement du modèle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=3, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.001, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.1s\n",
      "Meilleurs paramètres trouvés :\n",
      "{'learning_rate': 0.001, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Meilleur score de validation croisée :\n",
      "0.8607723577235772\n",
      "Rapport de classification sur l'ensemble de test :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.90      1.00      0.95        26\n",
      "\n",
      "    accuracy                           0.90        29\n",
      "   macro avg       0.45      0.50      0.47        29\n",
      "weighted avg       0.80      0.90      0.85        29\n",
      "\n",
      "Enregistrement du modèle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machines\n",
    "param_grid_gbc = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.001],\n",
    "    'min_samples_split': [2, 3, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search_gbc = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid_gbc, cv=3, scoring='accuracy', verbose=2)\n",
    "grid_search_gbc.fit(x_train_scaled, y_train)  # entraînement \n",
    "\n",
    "print(\"Meilleurs paramètres trouvés :\")\n",
    "print(grid_search_gbc.best_params_)\n",
    "print(\"Meilleur score de validation croisée :\")\n",
    "print(grid_search_gbc.best_score_)\n",
    "\n",
    "best_model_gbc = grid_search_gbc.best_estimator_\n",
    "y_pred_gbc = best_model_gbc.predict(x_test_scaled)\n",
    "\n",
    "print(\"Rapport de classification sur l'ensemble de test :\")\n",
    "print(classification_report(y_test, y_pred_gbc))\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "with open('best_gbm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model_gbc, file)\n",
    "print(\"Enregistrement du modèle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] END .......................C=0.1, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=0.1, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=0.1, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=1, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END ..........................C=1, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=10, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END .........................C=10, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=1, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=1, gamma=auto; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=2, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=2, gamma=auto; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END .......................C=100, degree=3, gamma=scale; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=3, gamma=auto; total time=   0.0s\n",
      "[CV] END ........................C=100, degree=3, gamma=auto; total time=   0.0s\n",
      "Meilleurs paramètres trouvés:\n",
      "{'C': 0.1, 'degree': 1, 'gamma': 'scale'}\n",
      "Meilleur score de validation croisée :\n",
      "0.8607723577235772\n",
      "Rapport de classification sur l'ensemble de test :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.90      1.00      0.95        26\n",
      "\n",
      "    accuracy                           0.90        29\n",
      "   macro avg       0.45      0.50      0.47        29\n",
      "weighted avg       0.80      0.90      0.85        29\n",
      "\n",
      "Enregistrement du modèle \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [1, 2, 3]\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=3, scoring='accuracy', verbose=2)\n",
    "grid_search_svm.fit(x_train_scaled, y_train)  # entraînement \n",
    "\n",
    "print(\"Meilleurs paramètres trouvés:\")\n",
    "print(grid_search_svm.best_params_)\n",
    "print(\"Meilleur score de validation croisée :\")\n",
    "print(grid_search_svm.best_score_)\n",
    "\n",
    "best_model_svm = grid_search_svm.best_estimator_\n",
    "y_pred_svm = best_model_svm.predict(x_test_scaled)\n",
    "\n",
    "print(\"Rapport de classification sur l'ensemble de test :\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Enregistrement du meilleur modèle\n",
    "with open('best_svm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model_svm, file)\n",
    "print(\"Enregistrement du modèle \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Fonctionnalité supplémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des données JSON réussie.\n",
      "Clés du DataFrame : Index(['forecastday'], dtype='object')\n",
      "                                         forecastday\n",
      "0  {'date': '2024-06-05', 'date_epoch': 171754560...\n",
      "                                         forecastday\n",
      "0  {'date': '2024-06-05', 'date_epoch': 171754560...\n",
      "                                         forecastday\n",
      "0  {'date': '2024-06-05', 'date_epoch': 171754560...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_weather_data(latitude, longitude, date):\n",
    "    api_key = \"ce3e5b714f4a4198b9585634242006\"\n",
    "    # URL pour récupérer les données météorologiques \n",
    "    url = f\"http://api.weatherapi.com/v1/history.json?key={api_key}&q={latitude},{longitude}&dt={date}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:     # si la réponse est OK (200), retourner les données JSON\n",
    "        print(\"Recupération des données json réussie, voici le lien \", url)\n",
    "        data = response.json() \n",
    "        weather = pd.DataFrame(data)\n",
    "        print(\"Clés du DataFrame:\", data['location'].keys())\n",
    "        print(\"Clés du DataFrame:\", data['forecast'].values())\n",
    "        #print(\"Valeur du DataFrame:\", data.values())\n",
    "        return weather\n",
    "    else:\n",
    "        print(\"Erreur lors de l'appel à l'API\")\n",
    "\n",
    "#exemple\n",
    "latitude = 48.8566\n",
    "longitude = 2.3522\n",
    "date = \"2024-06-05\"\n",
    "weather_data = get_weather_data(latitude, longitude, date)\n",
    "print(weather_data)\n",
    "\n",
    "if weather_data is not None:\n",
    "    print(weather_data.head())  # Affiche les premières lignes du DataFrame\n",
    "else:\n",
    "    print(\"Pas de données disponibles.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weather_data = get_weather_data(latitude, longitude, date)\n",
    "\n",
    "if weather_data is not None:\n",
    "    print(weather_data.head())  # Afficher les premières lignes du DataFrame\n",
    "else:\n",
    "    print(\"Pas de données disponibles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
